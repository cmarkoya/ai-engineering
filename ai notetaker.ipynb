{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cmarkoya/ai-engineering/blob/main/ai%20notetaker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWZHNb9Oqymt"
      },
      "source": [
        "# Note\n",
        "Llama 3.1 is a gated model which means you will have to gain access to this model from huggingface before you are able to use it in this notebook.\n",
        "\n",
        "Steps to take:\n",
        "1. Follow this link to go to Llama 3.1 documentation on huggingface https://huggingface.co/meta-llama/Llama-3.1-8B\n",
        "2. Fill in your details and request for access (this might take a few hours or a few days)\n",
        "3. You will receive an email from huggingface that you now have access to this family of models\n",
        "4. You can now access the model in your notebook after logging in to huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6seecUJHqxSx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e28a8c-ab71-4580-8a5b-382e009f1603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers diffusers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH7kp04UrDOA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(token=userdata.get(\"HF_TOKEN\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Pipeline Method\n",
        "\n",
        "## 1. Text Generation"
      ],
      "metadata": {
        "id": "5AhxWhaI3-L2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\")\n",
        "gen_output = generator(\"The secret to baking a good cake is \", max_length=200)[0]['generated_text']\n",
        "print('\\nOutput: \\n',gen_output)"
      ],
      "metadata": {
        "id": "oSGJPK8B38By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Text Summarization"
      ],
      "metadata": {
        "id": "Eka6d9iQgpVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If using a CPU, remove device=\"cuda\"\n",
        "summarizer = pipeline(\"summarization\", device=\"cuda\")\n",
        "sum_output = summarizer(gen_output, max_length=50)\n",
        "print(sum_output)"
      ],
      "metadata": {
        "id": "PGPCzIWqcMPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Text Classification"
      ],
      "metadata": {
        "id": "qFakiOi9midp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If using a CPU, remove device=\"cuda\"\n",
        "classifier = pipeline(model=\"ProsusAI/finbert\", device=\"cuda\")\n",
        "class_output = classifier(\"I really want this to be done\", max_length=100)\n",
        "print(class_output)"
      ],
      "metadata": {
        "id": "8FWeDcR3euqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Translation"
      ],
      "metadata": {
        "id": "07TZZvjgl6vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = pipeline(\"translation_en_to_fr\", model=\"google-t5/t5-base\", device=\"cuda\")\n",
        "translator_output = translator(\"Fly me to the moon\", max_length=100)\n",
        "print(translator_output)"
      ],
      "metadata": {
        "id": "9NuQaJlrkCKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Text-to-Image"
      ],
      "metadata": {
        "id": "HlPzfEqqpwnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "image_gen = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", device=\"cuda\")\n",
        "\n",
        "image_gen = image_gen(\"A fire cat riding on a spaceship\").images[0]\n",
        "image_gen"
      ],
      "metadata": {
        "id": "HvxqSB8oooll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All Tasks for the Transformers Library:** https://huggingface.co/docs/transformers/main_classes/pipelines\n",
        "\n",
        "**All Tasks for the Diffusers Library:** https://huggingface.co/docs/diffusers/using-diffusers/unconditional_image_generation"
      ],
      "metadata": {
        "id": "gdmV2sxIb8Mq"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}